{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Spectrogram of the recorded audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/73/bb9c093882d647437a9e6e87c7e6592d2df852f83ffac6f348b878979be0/pydub-0.23.0-py2.py3-none-any.whl\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paridhi\\Anaconda3\\lib\\site-packages\\pydub\\utils.py:165: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram\n",
    "\n",
    "### What is Spectrogram?? \n",
    "A visual representation of the spectrum of frequencies of sound or other signal as they vary with time. When the data is represented in a 3D plot they may be called waterfalls. The graph below is an example, the x-axis represents time and the y-axis is the frequency.\n",
    "\n",
    "<img src=\"images/Spectrogram-19thC.png\">\n",
    "(Source : Wikipedia)\n",
    "\n",
    "### Why do we need it?\n",
    "Spectrograms of audio, as in our case, can be used to identify spoken words phonetically, and to analyse the various changes in frequency when the Trigger word is spoken in the presence of other background frequencies. Our recorded signal (.wav) file is a time signal and the Spectrogram is a time-frequency graph. We convert the time signal to the time-frequency signal by using Fourier Transform. Fourier Transform takes the time signal and decomposes into the frequency frames, using complex mathematics. A Digital Signal Processing course has a very profound analysis on the Transformations and their Inverse Transformations to get back the time signal.\n",
    "\n",
    "But if you haven't taken the course, worry not, Python has inbuilt libraries to handle the conversion on its own!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot spectrogram for a wav audio file\n",
    "def graph_spectrogram(wav_file):\n",
    "    rate, data = get_wav_info(wav_file)\n",
    "    nfft = 200 # Length of each window segment\n",
    "    fs = 8000 # Sampling frequencies\n",
    "    noverlap = 120 # Overlap between windows\n",
    "    nchannels = data.ndim\n",
    "    if nchannels == 1:\n",
    "        pxx, freqs, bins, im = plt.specgram(data, nfft, fs, noverlap = noverlap)\n",
    "    elif nchannels == 2:\n",
    "        pxx, freqs, bins, im = plt.specgram(data[:,0], nfft, fs, noverlap = noverlap)\n",
    "    return pxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a wav file\n",
    "def get_wav_info(wav_file):\n",
    "    rate, data = wavfile.read(wav_file)\n",
    "    return rate, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to standardize volume of audio clip\n",
    "def match_target_amplitude(sound, target_dBFS):\n",
    "    change_in_dBFS = target_dBFS - sound.dBFS\n",
    "    return sound.apply_gain(change_in_dBFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw audio files for speech synthesis\n",
    "def load_raw_audio():\n",
    "    activates = []\n",
    "    backgrounds = []\n",
    "    negatives = []\n",
    "    for filename in os.listdir(\"./raw_data/activates\"):\n",
    "        if filename.endswith(\"wav\"):\n",
    "            activate = AudioSegment.from_wav(\"./raw_data/activates/\"+filename)\n",
    "            activates.append(activate)\n",
    "    for filename in os.listdir(\"./raw_data/backgrounds\"):\n",
    "        if filename.endswith(\"wav\"):\n",
    "            background = AudioSegment.from_wav(\"./raw_data/backgrounds/\"+filename)\n",
    "            backgrounds.append(background)\n",
    "    for filename in os.listdir(\"./raw_data/negatives\"):\n",
    "        if filename.endswith(\"wav\"):\n",
    "            negative = AudioSegment.from_wav(\"./raw_data/negatives/\"+filename)\n",
    "            negatives.append(negative)\n",
    "    return activates, negatives, backgrounds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
